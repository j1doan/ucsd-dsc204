{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ceb78c2",
   "metadata": {},
   "source": [
    "# Taxi Fare Prediction with a Generalized Additive Model (GAM)\n",
    "\n",
    "**Goal:** Predict NYC yellow taxi `fare_amount` from *non-fare* trip features — trip distance,\n",
    "trip duration (derived), hour of day, day of week, and passenger count — using a Gaussian GAM\n",
    "with identity link (`pygam.LinearGAM`).\n",
    "\n",
    "The notebook loads raw trip-level Parquet data, engineers time-based features, fits a GAM\n",
    "with smooth spline terms, evaluates on a held-out validation set, and interprets each\n",
    "predictor's partial-dependence (term-effect) curve with 95 % confidence bands.\n",
    "\n",
    "**Extra credit included:**\n",
    "- Location PCA enrichment: HW2 PC1/PC2 scores joined on pickup zone as smooth GAM terms\n",
    "- Bootstrap confidence-interval comparison: bootstrap-derived 95 % bands vs. pygam built-in CIs\n",
    "- Per-term fare component breakdown chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84065956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── SciPy compatibility shim for pygam ('csr_matrix has no attribute A') ──\n",
    "import scipy.sparse as _sp\n",
    "\n",
    "for _cls in (_sp.csr_matrix, _sp.csc_matrix):\n",
    "    if not hasattr(_cls, 'A'):\n",
    "        _cls.A = property(lambda self: self.toarray())\n",
    "\n",
    "# Make parent directory importable (for pivot_utils helpers)\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "try:\n",
    "    from pygam import LinearGAM, s, l\n",
    "    print('pygam imported successfully')\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        'pygam not found. Install with: pip install pygam\\n'\n",
    "        'If scipy csr_matrix errors appear, the shim above should handle them.'\n",
    "    )\n",
    "\n",
    "print('All imports OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════  CONFIGURATION  ═══════════════════════════════\n",
    "\n",
    "# Path to trip-level Parquet file (local path or S3 URI).\n",
    "# Public TLC data accessed anonymously via s3fs.\n",
    "DATA_PATH = 's3://dsc291-ucsd/taxi/Dataset/2021/yellow_taxi/yellow_tripdata_2021-01.parquet'\n",
    "\n",
    "# Maximum rows to load after reading (None = use all rows in file).\n",
    "MAX_ROWS = 100_000\n",
    "\n",
    "# Reproducibility seed (used for sampling and train/val split).\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fraction of clean data to use for training; remainder goes to validation.\n",
    "TRAIN_FRAC = 0.80\n",
    "\n",
    "# Path to HW2 PC scores CSV (pickup_place → pc1, pc2, …)\n",
    "PC_SCORES_PATH = '../hw2_output/pc_scores_by_pickup_place.csv'\n",
    "\n",
    "# Number of bootstrap resamples for extra-credit CI comparison.\n",
    "N_BOOTSTRAP = 50\n",
    "\n",
    "# ── Extra-credit toggles ──────────────────────────────────────────────────\n",
    "USE_LOCATION_PCA  = True   # Enrich features with HW2 PCA scores for pickup zone\n",
    "DO_BOOTSTRAP      = True   # Compare bootstrap CIs vs. pygam built-in CIs\n",
    "DO_FARE_BREAKDOWN = True   # Show per-term fare-component bar chart\n",
    "# ═════════════════════════════════════════════════════════════════════════\n",
    "print('Configuration loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d886623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 3: Load and Prepare Data ─────────────────────────────────────\n",
    "\n",
    "def _find_col(columns, *patterns):\n",
    "    \"\"\"Return first column matching any regex pattern (case-insensitive).\"\"\"\n",
    "    import re\n",
    "    for pat in patterns:\n",
    "        rx = re.compile(pat, re.IGNORECASE)\n",
    "        for c in columns:\n",
    "            if rx.fullmatch(str(c).strip()):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_trips(path, max_rows=None, seed=42):\n",
    "    \"\"\"Load trip-level parquet; handle local or S3 paths; sample if max_rows set.\"\"\"\n",
    "    storage_options = None\n",
    "    if isinstance(path, str) and path.startswith('s3://'):\n",
    "        storage_options = {'anon': True}\n",
    "    df = pd.read_parquet(path, storage_options=storage_options)\n",
    "    if max_rows is not None and len(df) > max_rows:\n",
    "        df = df.sample(n=max_rows, random_state=seed)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_columns(df):\n",
    "    \"\"\"Detect and rename key columns to unified names; raise if required cols missing.\"\"\"\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # ── datetime columns ──\n",
    "    pickup_col  = _find_col(cols, r'tpep_pickup_datetime', r'pickup_datetime',\n",
    "                             r'trip_pickup_datetime', r'.*pickup.*datetime.*')\n",
    "    dropoff_col = _find_col(cols, r'tpep_dropoff_datetime', r'dropoff_datetime',\n",
    "                             r'trip_dropoff_datetime', r'.*dropoff.*datetime.*')\n",
    "\n",
    "    # ── numeric feature columns ──\n",
    "    dist_col    = _find_col(cols, r'trip_distance', r'trip_dist', r'.*distance.*')\n",
    "    pax_col     = _find_col(cols, r'passenger_count', r'passengers', r'pax_count')\n",
    "    fare_col    = _find_col(cols, r'fare_amount', r'fare_amt', r'fare')\n",
    "    loc_col     = _find_col(cols, r'pulocationid', r'pu_location_id', r'pickup_location',\n",
    "                             r'pickup_place', r'pulocation')\n",
    "\n",
    "    missing = [name for name, col in\n",
    "               [('pickup_datetime', pickup_col), ('dropoff_datetime', dropoff_col),\n",
    "                ('trip_distance',   dist_col),   ('fare_amount',      fare_col)]\n",
    "               if col is None]\n",
    "    if missing:\n",
    "        raise ValueError(f'Could not find required columns: {missing}.\\nAvailable: {cols}')\n",
    "\n",
    "    rename = {pickup_col: 'pickup_datetime', dropoff_col: 'dropoff_datetime',\n",
    "              dist_col: 'trip_distance', fare_col: 'fare_amount'}\n",
    "    if pax_col:\n",
    "        rename[pax_col] = 'passenger_count'\n",
    "    if loc_col:\n",
    "        rename[loc_col] = 'pickup_location'\n",
    "\n",
    "    df = df.rename(columns=rename)\n",
    "\n",
    "    # Keep only recognized columns\n",
    "    keep = ['pickup_datetime', 'dropoff_datetime', 'trip_distance',\n",
    "            'fare_amount'] + (\n",
    "           ['passenger_count'] if 'passenger_count' in df.columns else []) + (\n",
    "           ['pickup_location'] if 'pickup_location' in df.columns else [])\n",
    "    return df[[c for c in keep if c in df.columns]]\n",
    "\n",
    "\n",
    "print('Loading data from:', DATA_PATH)\n",
    "raw_df = load_trips(DATA_PATH, max_rows=MAX_ROWS, seed=RANDOM_SEED)\n",
    "print(f'  Loaded {len(raw_df):,} rows, {len(raw_df.columns)} columns.')\n",
    "print('  Columns:', list(raw_df.columns))\n",
    "\n",
    "df = normalize_columns(raw_df)\n",
    "print(f'  After normalization: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 3 (cont.): Feature Engineering and Cleaning ──────────────────\n",
    "\n",
    "# Parse datetimes\n",
    "df['pickup_datetime']  = pd.to_datetime(df['pickup_datetime'],  errors='coerce')\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'], errors='coerce')\n",
    "\n",
    "# Derived temporal features\n",
    "df['trip_duration_min'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60.0\n",
    "df['hour_of_day']       = df['pickup_datetime'].dt.hour\n",
    "df['day_of_week']       = df['pickup_datetime'].dt.dayofweek   # 0=Monday … 6=Sunday\n",
    "\n",
    "# Fill missing passenger_count with 1\n",
    "if 'passenger_count' not in df.columns:\n",
    "    df['passenger_count'] = 1\n",
    "df['passenger_count'] = pd.to_numeric(df['passenger_count'], errors='coerce').fillna(1).clip(lower=1)\n",
    "\n",
    "# ── Data Cleaning ──\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['pickup_datetime', 'dropoff_datetime', 'trip_distance',\n",
    "                        'fare_amount', 'trip_duration_min'])\n",
    "df = df[\n",
    "    (df['fare_amount']      > 0) &\n",
    "    (df['trip_distance']    > 0) &\n",
    "    (df['trip_duration_min'] > 0)\n",
    "]\n",
    "\n",
    "# Cap extreme values at 99th percentile\n",
    "for col in ['fare_amount', 'trip_distance', 'trip_duration_min']:\n",
    "    p99 = df[col].quantile(0.99)\n",
    "    df = df[df[col] <= p99]\n",
    "\n",
    "after = len(df)\n",
    "print(f'Rows before cleaning: {before:,}  →  after cleaning: {after:,}  (removed {before - after:,})')\n",
    "\n",
    "print('\\nFeature summary:')\n",
    "display(df[['fare_amount', 'trip_distance', 'trip_duration_min',\n",
    "            'hour_of_day', 'day_of_week', 'passenger_count']].describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caec03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section (Extra Credit): Location PCA Enrichment ───────────────────────\n",
    "# Join HW2 PC1/PC2 scores by pickup zone so the GAM can absorb\n",
    "# spatial structure via smooth terms on PC1 and PC2.\n",
    "\n",
    "location_pca_joined = False\n",
    "\n",
    "if USE_LOCATION_PCA and 'pickup_location' in df.columns:\n",
    "    try:\n",
    "        pc_scores = pd.read_csv(PC_SCORES_PATH)\n",
    "        # Rename index col to pickup_location for the join\n",
    "        first_col = pc_scores.columns[0]\n",
    "        if first_col != 'pickup_location':\n",
    "            pc_scores = pc_scores.rename(columns={first_col: 'pickup_location'})\n",
    "\n",
    "        # Try to coerce both sides to the same type for join\n",
    "        df['pickup_location'] = pd.to_numeric(df['pickup_location'], errors='coerce')\n",
    "        pc_scores['pickup_location'] = pd.to_numeric(pc_scores['pickup_location'], errors='coerce')\n",
    "\n",
    "        pc_sub = pc_scores[['pickup_location', 'pc1', 'pc2']].rename(\n",
    "            columns={'pc1': 'pc1_score', 'pc2': 'pc2_score'})\n",
    "\n",
    "        df = df.merge(pc_sub, on='pickup_location', how='left')\n",
    "\n",
    "        matched = df['pc1_score'].notna().sum()\n",
    "        print(f'PCA scores joined: {matched:,}/{len(df):,} rows matched a pickup zone.')\n",
    "\n",
    "        if matched / len(df) >= 0.10:   # at least 10 % matched\n",
    "            # Fill unmatched rows with median so no rows are dropped\n",
    "            for col in ['pc1_score', 'pc2_score']:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            location_pca_joined = True\n",
    "            print('Location PCA enrichment ENABLED (pc1_score, pc2_score added to feature matrix).')\n",
    "        else:\n",
    "            df = df.drop(columns=['pc1_score', 'pc2_score'], errors='ignore')\n",
    "            print('Match rate too low — skipping location PCA enrichment.')\n",
    "    except Exception as e:\n",
    "        print(f'Could not load PC scores ({e}) — skipping location PCA enrichment.')\n",
    "else:\n",
    "    if not USE_LOCATION_PCA:\n",
    "        print('USE_LOCATION_PCA=False — skipping.')\n",
    "    else:\n",
    "        print('No pickup_location column found — skipping location PCA enrichment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 3 (cont.): Train / Validation Split ───────────────────────────\n",
    "\n",
    "# Build feature list in a fixed, documented order\n",
    "FEATURE_NAMES = ['trip_distance', 'trip_duration_min', 'hour_of_day',\n",
    "                 'day_of_week', 'passenger_count']\n",
    "if location_pca_joined:\n",
    "    FEATURE_NAMES += ['pc1_score', 'pc2_score']\n",
    "\n",
    "TARGET = 'fare_amount'\n",
    "\n",
    "X = df[FEATURE_NAMES].values\n",
    "y = df[TARGET].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size=TRAIN_FRAC, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f'Feature matrix: {X.shape[1]} features → {FEATURE_NAMES}')\n",
    "print(f'Train size: {len(X_train):,}   |   Validation size: {len(X_val):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b42664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 4: Define and Fit the GAM ─────────────────────────────────────\n",
    "# Smooth terms (s) for continuous/cyclic predictors; linear term (l) for\n",
    "# passenger_count (discrete, small range).\n",
    "#\n",
    "# Feature index map:\n",
    "#  0: trip_distance     → s(0)  cyclic=False\n",
    "#  1: trip_duration_min → s(1)  cyclic=False\n",
    "#  2: hour_of_day       → s(2)  cyclic=True  (0..23 wraps)\n",
    "#  3: day_of_week       → s(3)  cyclic=True  (0..6 wraps)\n",
    "#  4: passenger_count   → l(4)  linear\n",
    "#  5: pc1_score         → s(5)  [if location_pca_joined]\n",
    "#  6: pc2_score         → s(6)  [if location_pca_joined]\n",
    "\n",
    "terms = s(0) + s(1) + s(2, n_splines=24, spline_order=3) + s(3, n_splines=7) + l(4)\n",
    "if location_pca_joined:\n",
    "    terms = terms + s(5) + s(6)\n",
    "\n",
    "gam = LinearGAM(terms, fit_intercept=True)\n",
    "\n",
    "# Simple lambda grid search over log-space values\n",
    "gam.gridsearch(X_train, y_train, progress=False)\n",
    "\n",
    "print('GAM fit complete.')\n",
    "print(gam.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21536c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 5: Evaluate ────────────────────────────────────────────────────\n",
    "\n",
    "y_pred = gam.predict(X_val)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae  = mean_absolute_error(y_val, y_pred)\n",
    "r2   = r2_score(y_val, y_pred)\n",
    "\n",
    "metrics_df = pd.DataFrame({'Metric': ['RMSE ($)', 'MAE ($)', 'R²'],\n",
    "                            'Value':  [f'{rmse:.4f}', f'{mae:.4f}', f'{r2:.4f}']})\n",
    "print('Validation set metrics:')\n",
    "display(metrics_df.to_string(index=False))\n",
    "\n",
    "# ── Actual vs. Predicted scatter plot ─────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.scatter(y_val, y_pred, alpha=0.15, s=8, color='steelblue', label='Predictions')\n",
    "lims = [min(y_val.min(), y_pred.min()), max(y_val.max(), y_pred.max())]\n",
    "ax.plot(lims, lims, 'r--', linewidth=1.5, label='y = x (perfect)')\n",
    "ax.set_xlabel('Actual Fare ($)')\n",
    "ax.set_ylabel('Predicted Fare ($)')\n",
    "ax.set_title(f'Actual vs Predicted Fare (Validation Set)\\nRMSE={rmse:.2f}  MAE={mae:.2f}  R²={r2:.4f}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('actual_vs_predicted.png', dpi=120)\n",
    "plt.show()\n",
    "print('Scatter plot saved to hw3_output/actual_vs_predicted.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12717c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 6: Partial Dependence (Term Effect) Plots ─────────────────────\n",
    "# Plots how each predictor relates to fare while holding others at a reference\n",
    "# value.  95 % confidence intervals shown as shaded bands.\n",
    "\n",
    "TERM_LABELS = {\n",
    "    0: ('trip_distance',     'Trip Distance (miles)'),\n",
    "    1: ('trip_duration_min', 'Trip Duration (minutes)'),\n",
    "    2: ('hour_of_day',       'Hour of Day'),\n",
    "    3: ('day_of_week',       'Day of Week (0=Mon)'),\n",
    "    4: ('passenger_count',   'Passenger Count'),\n",
    "}\n",
    "if location_pca_joined:\n",
    "    TERM_LABELS[5] = ('pc1_score', 'Pickup Zone PC1 Score')\n",
    "    TERM_LABELS[6] = ('pc2_score', 'Pickup Zone PC2 Score')\n",
    "\n",
    "n_terms = len(TERM_LABELS)\n",
    "ncols = 3\n",
    "nrows = (n_terms + ncols - 1) // ncols\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4 * nrows))\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for idx, (term_i, (feat_name, xlabel)) in enumerate(TERM_LABELS.items()):\n",
    "    ax = axes[idx]\n",
    "    XX = gam.generate_X_grid(term=term_i)\n",
    "    pdep, confi = gam.partial_dependence(term=term_i, X=XX, width=0.95)\n",
    "    ax.plot(XX[:, term_i], pdep, color='steelblue', linewidth=2, label='Partial effect')\n",
    "    ax.fill_between(XX[:, term_i], confi[:, 0], confi[:, 1],\n",
    "                    alpha=0.3, color='steelblue', label='95% CI')\n",
    "    ax.axhline(0, color='grey', linewidth=0.8, linestyle='--')\n",
    "    ax.set_xlabel(xlabel, fontsize=11)\n",
    "    ax.set_ylabel('Partial Effect on Fare ($)', fontsize=10)\n",
    "    ax.set_title(f'Effect of {feat_name}', fontsize=12)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "# Hide any unused subplot panels\n",
    "for j in range(n_terms, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('GAM Partial Dependence Plots (95% CI)', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('partial_dependence.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Partial dependence plots saved to hw3_output/partial_dependence.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Extra Credit: Bootstrap CI Comparison ─────────────────────────────────\n",
    "# For 3 key terms (trip_distance, trip_duration_min, hour_of_day), we compare:\n",
    "#   (a) pygam's built-in 95 % CI (computed from the GAM covariance matrix)\n",
    "#   (b) Bootstrap 95 % CI (percentile method, N_BOOTSTRAP resamples)\n",
    "# This lets us assess whether pygam's analytical CIs are realistic.\n",
    "\n",
    "if not DO_BOOTSTRAP:\n",
    "    print('DO_BOOTSTRAP=False — skipping.')\n",
    "else:\n",
    "    BOOT_TERMS = [0, 1, 2]   # indices: trip_distance, trip_duration_min, hour_of_day\n",
    "    BOOT_LABELS = ['trip_distance', 'trip_duration_min', 'hour_of_day']\n",
    "\n",
    "    # Pre-compute evaluation grids from the full GAM\n",
    "    boot_grids   = [gam.generate_X_grid(term=t) for t in BOOT_TERMS]\n",
    "    # Storage: list of arrays shape (N_BOOTSTRAP, n_grid_points)\n",
    "    boot_curves  = [[] for _ in BOOT_TERMS]\n",
    "\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    n_train = len(X_train)\n",
    "\n",
    "    print(f'Running {N_BOOTSTRAP} bootstrap fits (this may take ~1-2 min)…')\n",
    "    for b in range(N_BOOTSTRAP):\n",
    "        idx = rng.integers(0, n_train, size=n_train)   # resample with replacement\n",
    "        Xb, yb = X_train[idx], y_train[idx]\n",
    "        try:\n",
    "            gam_b = LinearGAM(terms, fit_intercept=True)\n",
    "            # Fix lambdas to those found by gridsearch (faster, more stable)\n",
    "            gam_b.lam = gam.lam\n",
    "            gam_b.fit(Xb, yb)\n",
    "            for k, (term_i, grid) in enumerate(zip(BOOT_TERMS, boot_grids)):\n",
    "                pdep_b, _ = gam_b.partial_dependence(term=term_i, X=grid, width=0.95)\n",
    "                boot_curves[k].append(pdep_b)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if (b + 1) % 10 == 0:\n",
    "            print(f'  {b + 1}/{N_BOOTSTRAP} done')\n",
    "\n",
    "    print('Bootstrap complete.  Plotting CI comparison…')\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(BOOT_TERMS), figsize=(7 * len(BOOT_TERMS), 5))\n",
    "    if len(BOOT_TERMS) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for k, (term_i, feat_label) in enumerate(zip(BOOT_TERMS, BOOT_LABELS)):\n",
    "        ax = axes[k]\n",
    "        grid = boot_grids[k]\n",
    "        x_vals = grid[:, term_i]\n",
    "\n",
    "        # pygam analytic CI\n",
    "        pdep_main, confi_main = gam.partial_dependence(term=term_i, X=grid, width=0.95)\n",
    "\n",
    "        # Bootstrap percentile CI\n",
    "        curves_arr = np.array(boot_curves[k])  # shape (n_valid, n_grid)\n",
    "        boot_lo = np.percentile(curves_arr, 2.5,  axis=0)\n",
    "        boot_hi = np.percentile(curves_arr, 97.5, axis=0)\n",
    "\n",
    "        ax.plot(x_vals, pdep_main, color='steelblue', linewidth=2, label='GAM estimate')\n",
    "        ax.fill_between(x_vals, confi_main[:, 0], confi_main[:, 1],\n",
    "                        alpha=0.30, color='steelblue', label='pygam 95% CI')\n",
    "        ax.fill_between(x_vals, boot_lo, boot_hi,\n",
    "                        alpha=0.25, color='orange', label=f'Bootstrap 95% CI (B={len(curves_arr)})')\n",
    "        ax.set_xlabel(feat_label, fontsize=11)\n",
    "        ax.set_ylabel('Partial Effect ($)')\n",
    "        ax.set_title(f'CI Comparison: {feat_label}', fontsize=12)\n",
    "        ax.legend(fontsize=9)\n",
    "\n",
    "    plt.suptitle('Bootstrap vs. pygam Built-in 95% Confidence Intervals', fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bootstrap_ci_comparison.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Bootstrap CI comparison saved to hw3_output/bootstrap_ci_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb625a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Extra Credit: Per-Term Fare Component Breakdown ────────────────────────\n",
    "# Quantify how much each predictor contributes to the average predicted fare.\n",
    "# For each term i, we evaluate its partial_dependence at the training-set mean\n",
    "# of X and report the absolute contribution (additive effect).\n",
    "\n",
    "if not DO_FARE_BREAKDOWN:\n",
    "    print('DO_FARE_BREAKDOWN=False — skipping.')\n",
    "else:\n",
    "    # Use the validation set to measure average partial contributions\n",
    "    contributions = {}\n",
    "    for term_i, (feat_name, xlabel) in TERM_LABELS.items():\n",
    "        pdep, _ = gam.partial_dependence(term=term_i, X=X_val, width=0.95)\n",
    "        contributions[feat_name] = float(np.mean(np.abs(pdep)))\n",
    "\n",
    "    # Also include the fitted intercept as a separate component\n",
    "    intercept_val = float(gam.coef_[-1])   # last coefficient is intercept\n",
    "\n",
    "    labels = list(contributions.keys())\n",
    "    values = list(contributions.values())\n",
    "\n",
    "    # Sort descending\n",
    "    sorted_pairs = sorted(zip(values, labels), reverse=True)\n",
    "    values_sorted, labels_sorted = zip(*sorted_pairs)\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 0.8, len(labels_sorted)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    bars = ax.barh(labels_sorted, values_sorted, color=colors, edgecolor='white')\n",
    "    ax.set_xlabel('Mean |Partial Effect| on Fare ($)', fontsize=11)\n",
    "    ax.set_title('Per-Term Fare Component Breakdown\\n(mean absolute partial effect on validation set)', fontsize=12)\n",
    "    for bar, v in zip(bars, values_sorted):\n",
    "        ax.text(v + 0.02, bar.get_y() + bar.get_height() / 2,\n",
    "                f'${v:.2f}', va='center', fontsize=10)\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fare_component_breakdown.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print('\\nFare Component Breakdown (mean |partial effect| on validation set):')\n",
    "    for feat, val in sorted(contributions.items(), key=lambda x: -x[1]):\n",
    "        print(f'  {feat:25s}: ${val:.3f}')\n",
    "    print(f'  {\"intercept\":25s}: ${intercept_val:.3f}')\n",
    "    print('Component chart saved to hw3_output/fare_component_breakdown.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f25e0",
   "metadata": {},
   "source": [
    "## Discussion: Limitations\n",
    "\n",
    "The GAM provides a transparent and interpretable model for taxi fare prediction, but several\n",
    "limitations constrain its real-world accuracy:\n",
    "\n",
    "1. **Missing fare components**: The model intentionally excludes tolls, MTA taxes, congestion\n",
    "   surcharges, and airport fees — all of which add fixed or route-dependent charges to the\n",
    "   actual meter reading.  A trip through a toll zone will therefore be systematically\n",
    "   under-predicted.\n",
    "\n",
    "2. **No fine-grained GPS location**: Pickup/drop-off coordinates or street segments are not\n",
    "   used directly; instead, location is represented only through aggregate PCA scores derived\n",
    "   from the ride-count wide table.  Route geometry (e.g. highway vs. local streets) and\n",
    "   traffic conditions are therefore not captured.\n",
    "\n",
    "3. **Single-month snapshot**: The model is trained on January 2021 data only.  Seasonal demand\n",
    "   patterns, holiday effects, and post-pandemic ridership recovery trends visible in later\n",
    "   months are absent.\n",
    "\n",
    "4. **Passenger-count noise**: Many NYC TLC records have `passenger_count = 0` or suspiciously\n",
    "   high values; even after cleaning, this feature has low predictive power for fare."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
