{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ceb78c2",
   "metadata": {},
   "source": [
    "# Taxi Fare Prediction with a Generalized Additive Model (GAM)\n",
    "\n",
    "**Goal:** Predict NYC yellow taxi `fare_amount` from *non-fare* trip features — trip distance,\n",
    "trip duration (derived), hour of day, day of week, and passenger count — using a Gaussian GAM\n",
    "with identity link (`pygam.LinearGAM`).\n",
    "\n",
    "The notebook loads raw trip-level Parquet data, engineers time-based features, fits a GAM\n",
    "with smooth spline terms, evaluates on a held-out validation set, and interprets each\n",
    "predictor's partial-dependence (term-effect) curve with 95 % confidence bands.\n",
    "\n",
    "**Extra credit included (labeled [EC]):**\n",
    "- Location PCA enrichment: HW2 PC1/PC2 scores joined on pickup zone as smooth GAM terms\n",
    "- Bootstrap confidence-interval comparison: bootstrap-derived 95 % bands vs. pygam built-in CIs\n",
    "- Per-term fare component breakdown chart\n",
    "\n",
    "**EXTRA sections (beyond assignment scope — labeled [EXTRA]):**\n",
    "- **EXTRA 1 — Seaborn EDA:** Richer exploratory visuals (fare distribution, fare vs hour violin, correlation heatmap) built with seaborn\n",
    "- **EXTRA 2 — Region Analysis:** Assign all pickup/drop-off zones to NYC boroughs via lat/lon bounding boxes; compare fare distributions and fit per-borough GAMs to reveal spatial variation in the distance effect\n",
    "- **EXTRA 3 — Multi-year & Multi-service:** Load samples from Yellow 2015, 2021, 2022 and Green 2021; compare fare distributions (seaborn violin) and overlay GAM distance partial-dependence curves to show how fares and rate structures changed over time and across services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84065956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── SciPy compatibility shim for pygam ('csr_matrix has no attribute A') ──\n",
    "import scipy.sparse as _sp\n",
    "\n",
    "for _cls in (_sp.csr_matrix, _sp.csc_matrix):\n",
    "    if not hasattr(_cls, 'A'):\n",
    "        _cls.A = property(lambda self: self.toarray())\n",
    "\n",
    "# Make parent directory importable (for pivot_utils helpers)\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "try:\n",
    "    from pygam import LinearGAM, s, l\n",
    "    print('pygam imported successfully')\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        'pygam not found. Install with: pip install pygam\\n'\n",
    "        'If scipy csr_matrix errors appear, the shim above should handle them.'\n",
    "    )\n",
    "\n",
    "print('All imports OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════  CONFIGURATION  ═══════════════════════════════\n",
    "\n",
    "# Path to trip-level Parquet file (local path or S3 URI).\n",
    "# Public TLC data accessed anonymously via s3fs.\n",
    "DATA_PATH = 's3://dsc291-ucsd/taxi/Dataset/2021/yellow_taxi/yellow_tripdata_2021-01.parquet'\n",
    "\n",
    "# Maximum rows to load after reading (None = use all rows in file).\n",
    "MAX_ROWS = 100_000\n",
    "\n",
    "# Reproducibility seed (used for sampling and train/val split).\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fraction of clean data to use for training; remainder goes to validation.\n",
    "TRAIN_FRAC = 0.80\n",
    "\n",
    "# Path to HW2 PC scores CSV (pickup_place → pc1, pc2, …)\n",
    "PC_SCORES_PATH = '../hw2_output/pc_scores_by_pickup_place.csv'\n",
    "\n",
    "# Number of bootstrap resamples for extra-credit CI comparison.\n",
    "N_BOOTSTRAP = 50\n",
    "\n",
    "# ── Extra-credit toggles ──────────────────────────────────────────────────\n",
    "USE_LOCATION_PCA  = True   # Enrich features with HW2 PCA scores for pickup zone\n",
    "DO_BOOTSTRAP      = True   # Compare bootstrap CIs vs. pygam built-in CIs\n",
    "DO_FARE_BREAKDOWN = True   # Show per-term fare-component bar chart\n",
    "# ═════════════════════════════════════════════════════════════════════════\n",
    "print('Configuration loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d886623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 3: Load and Prepare Data ─────────────────────────────────────\n",
    "\n",
    "def _find_col(columns, *patterns):\n",
    "    \"\"\"Return first column matching any regex pattern (case-insensitive).\"\"\"\n",
    "    import re\n",
    "    for pat in patterns:\n",
    "        rx = re.compile(pat, re.IGNORECASE)\n",
    "        for c in columns:\n",
    "            if rx.fullmatch(str(c).strip()):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_trips(path, max_rows=None, seed=42):\n",
    "    \"\"\"Load trip-level parquet; handle local or S3 paths; sample if max_rows set.\"\"\"\n",
    "    storage_options = None\n",
    "    if isinstance(path, str) and path.startswith('s3://'):\n",
    "        storage_options = {'anon': True}\n",
    "    df = pd.read_parquet(path, storage_options=storage_options)\n",
    "    if max_rows is not None and len(df) > max_rows:\n",
    "        df = df.sample(n=max_rows, random_state=seed)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_columns(df):\n",
    "    \"\"\"Detect and rename key columns to unified names; raise if required cols missing.\"\"\"\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # ── datetime columns ──\n",
    "    pickup_col  = _find_col(cols, r'tpep_pickup_datetime', r'pickup_datetime',\n",
    "                             r'trip_pickup_datetime', r'.*pickup.*datetime.*')\n",
    "    dropoff_col = _find_col(cols, r'tpep_dropoff_datetime', r'dropoff_datetime',\n",
    "                             r'trip_dropoff_datetime', r'.*dropoff.*datetime.*')\n",
    "\n",
    "    # ── numeric feature columns ──\n",
    "    dist_col    = _find_col(cols, r'trip_distance', r'trip_dist', r'.*distance.*')\n",
    "    pax_col     = _find_col(cols, r'passenger_count', r'passengers', r'pax_count')\n",
    "    fare_col    = _find_col(cols, r'fare_amount', r'fare_amt', r'fare')\n",
    "    loc_col     = _find_col(cols, r'pulocationid', r'pu_location_id', r'pickup_location',\n",
    "                             r'pickup_place', r'pulocation')\n",
    "\n",
    "    missing = [name for name, col in\n",
    "               [('pickup_datetime', pickup_col), ('dropoff_datetime', dropoff_col),\n",
    "                ('trip_distance',   dist_col),   ('fare_amount',      fare_col)]\n",
    "               if col is None]\n",
    "    if missing:\n",
    "        raise ValueError(f'Could not find required columns: {missing}.\\nAvailable: {cols}')\n",
    "\n",
    "    rename = {pickup_col: 'pickup_datetime', dropoff_col: 'dropoff_datetime',\n",
    "              dist_col: 'trip_distance', fare_col: 'fare_amount'}\n",
    "    if pax_col:\n",
    "        rename[pax_col] = 'passenger_count'\n",
    "    if loc_col:\n",
    "        rename[loc_col] = 'pickup_location'\n",
    "\n",
    "    df = df.rename(columns=rename)\n",
    "\n",
    "    # Keep only recognized columns\n",
    "    keep = ['pickup_datetime', 'dropoff_datetime', 'trip_distance',\n",
    "            'fare_amount'] + (\n",
    "           ['passenger_count'] if 'passenger_count' in df.columns else []) + (\n",
    "           ['pickup_location'] if 'pickup_location' in df.columns else [])\n",
    "    return df[[c for c in keep if c in df.columns]]\n",
    "\n",
    "\n",
    "print('Loading data from:', DATA_PATH)\n",
    "raw_df = load_trips(DATA_PATH, max_rows=MAX_ROWS, seed=RANDOM_SEED)\n",
    "print(f'  Loaded {len(raw_df):,} rows, {len(raw_df.columns)} columns.')\n",
    "print('  Columns:', list(raw_df.columns))\n",
    "\n",
    "df = normalize_columns(raw_df)\n",
    "print(f'  After normalization: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 3 (cont.): Feature Engineering and Cleaning ──────────────────\n",
    "\n",
    "# Parse datetimes\n",
    "df['pickup_datetime']  = pd.to_datetime(df['pickup_datetime'],  errors='coerce')\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'], errors='coerce')\n",
    "\n",
    "# Derived temporal features\n",
    "df['trip_duration_min'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60.0\n",
    "df['hour_of_day']       = df['pickup_datetime'].dt.hour\n",
    "df['day_of_week']       = df['pickup_datetime'].dt.dayofweek   # 0=Monday … 6=Sunday\n",
    "\n",
    "# Fill missing passenger_count with 1\n",
    "if 'passenger_count' not in df.columns:\n",
    "    df['passenger_count'] = 1\n",
    "df['passenger_count'] = pd.to_numeric(df['passenger_count'], errors='coerce').fillna(1).clip(lower=1)\n",
    "\n",
    "# ── Data Cleaning ──\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['pickup_datetime', 'dropoff_datetime', 'trip_distance',\n",
    "                        'fare_amount', 'trip_duration_min'])\n",
    "df = df[\n",
    "    (df['fare_amount']      > 0) &\n",
    "    (df['trip_distance']    > 0) &\n",
    "    (df['trip_duration_min'] > 0)\n",
    "]\n",
    "\n",
    "# Cap extreme values at 99th percentile\n",
    "for col in ['fare_amount', 'trip_distance', 'trip_duration_min']:\n",
    "    p99 = df[col].quantile(0.99)\n",
    "    df = df[df[col] <= p99]\n",
    "\n",
    "after = len(df)\n",
    "print(f'Rows before cleaning: {before:,}  →  after cleaning: {after:,}  (removed {before - after:,})')\n",
    "\n",
    "print('\\nFeature summary:')\n",
    "display(df[['fare_amount', 'trip_distance', 'trip_duration_min',\n",
    "            'hour_of_day', 'day_of_week', 'passenger_count']].describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caec03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section (Extra Credit): Location PCA Enrichment ───────────────────────\n",
    "# Join HW2 PC1/PC2 scores by pickup zone so the GAM can absorb\n",
    "# spatial structure via smooth terms on PC1 and PC2.\n",
    "\n",
    "location_pca_joined = False\n",
    "\n",
    "if USE_LOCATION_PCA and 'pickup_location' in df.columns:\n",
    "    try:\n",
    "        pc_scores = pd.read_csv(PC_SCORES_PATH)\n",
    "        # Rename index col to pickup_location for the join\n",
    "        first_col = pc_scores.columns[0]\n",
    "        if first_col != 'pickup_location':\n",
    "            pc_scores = pc_scores.rename(columns={first_col: 'pickup_location'})\n",
    "\n",
    "        # Try to coerce both sides to the same type for join\n",
    "        df['pickup_location'] = pd.to_numeric(df['pickup_location'], errors='coerce')\n",
    "        pc_scores['pickup_location'] = pd.to_numeric(pc_scores['pickup_location'], errors='coerce')\n",
    "\n",
    "        pc_sub = pc_scores[['pickup_location', 'pc1', 'pc2']].rename(\n",
    "            columns={'pc1': 'pc1_score', 'pc2': 'pc2_score'})\n",
    "\n",
    "        df = df.merge(pc_sub, on='pickup_location', how='left')\n",
    "\n",
    "        matched = df['pc1_score'].notna().sum()\n",
    "        print(f'PCA scores joined: {matched:,}/{len(df):,} rows matched a pickup zone.')\n",
    "\n",
    "        if matched / len(df) >= 0.10:   # at least 10 % matched\n",
    "            # Fill unmatched rows with median so no rows are dropped\n",
    "            for col in ['pc1_score', 'pc2_score']:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            location_pca_joined = True\n",
    "            print('Location PCA enrichment ENABLED (pc1_score, pc2_score added to feature matrix).')\n",
    "        else:\n",
    "            df = df.drop(columns=['pc1_score', 'pc2_score'], errors='ignore')\n",
    "            print('Match rate too low — skipping location PCA enrichment.')\n",
    "    except Exception as e:\n",
    "        print(f'Could not load PC scores ({e}) — skipping location PCA enrichment.')\n",
    "else:\n",
    "    if not USE_LOCATION_PCA:\n",
    "        print('USE_LOCATION_PCA=False — skipping.')\n",
    "    else:\n",
    "        print('No pickup_location column found — skipping location PCA enrichment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 3 (cont.): Train / Validation Split ───────────────────────────\n",
    "\n",
    "# Build feature list in a fixed, documented order\n",
    "FEATURE_NAMES = ['trip_distance', 'trip_duration_min', 'hour_of_day',\n",
    "                 'day_of_week', 'passenger_count']\n",
    "if location_pca_joined:\n",
    "    FEATURE_NAMES += ['pc1_score', 'pc2_score']\n",
    "\n",
    "TARGET = 'fare_amount'\n",
    "\n",
    "X = df[FEATURE_NAMES].values\n",
    "y = df[TARGET].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size=TRAIN_FRAC, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f'Feature matrix: {X.shape[1]} features → {FEATURE_NAMES}')\n",
    "print(f'Train size: {len(X_train):,}   |   Validation size: {len(X_val):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b42664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 4: Define and Fit the GAM ─────────────────────────────────────\n",
    "# Smooth terms (s) for continuous/cyclic predictors; linear term (l) for\n",
    "# passenger_count (discrete, small range).\n",
    "#\n",
    "# Feature index map:\n",
    "#  0: trip_distance     → s(0)  cyclic=False\n",
    "#  1: trip_duration_min → s(1)  cyclic=False\n",
    "#  2: hour_of_day       → s(2)  cyclic=True  (0..23 wraps)\n",
    "#  3: day_of_week       → s(3)  cyclic=True  (0..6 wraps)\n",
    "#  4: passenger_count   → l(4)  linear\n",
    "#  5: pc1_score         → s(5)  [if location_pca_joined]\n",
    "#  6: pc2_score         → s(6)  [if location_pca_joined]\n",
    "\n",
    "terms = s(0) + s(1) + s(2, n_splines=24, spline_order=3) + s(3, n_splines=7) + l(4)\n",
    "if location_pca_joined:\n",
    "    terms = terms + s(5) + s(6)\n",
    "\n",
    "gam = LinearGAM(terms, fit_intercept=True)\n",
    "\n",
    "# Simple lambda grid search over log-space values\n",
    "gam.gridsearch(X_train, y_train, progress=False)\n",
    "\n",
    "print('GAM fit complete.')\n",
    "print(gam.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21536c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 5: Evaluate ────────────────────────────────────────────────────\n",
    "\n",
    "y_pred = gam.predict(X_val)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae  = mean_absolute_error(y_val, y_pred)\n",
    "r2   = r2_score(y_val, y_pred)\n",
    "\n",
    "metrics_df = pd.DataFrame({'Metric': ['RMSE ($)', 'MAE ($)', 'R²'],\n",
    "                            'Value':  [f'{rmse:.4f}', f'{mae:.4f}', f'{r2:.4f}']})\n",
    "print('Validation set metrics:')\n",
    "display(metrics_df.to_string(index=False))\n",
    "\n",
    "# ── Actual vs. Predicted scatter plot ─────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.scatter(y_val, y_pred, alpha=0.15, s=8, color='steelblue', label='Predictions')\n",
    "lims = [min(y_val.min(), y_pred.min()), max(y_val.max(), y_pred.max())]\n",
    "ax.plot(lims, lims, 'r--', linewidth=1.5, label='y = x (perfect)')\n",
    "ax.set_xlabel('Actual Fare ($)')\n",
    "ax.set_ylabel('Predicted Fare ($)')\n",
    "ax.set_title(f'Actual vs Predicted Fare (Validation Set)\\nRMSE={rmse:.2f}  MAE={mae:.2f}  R²={r2:.4f}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('actual_vs_predicted.png', dpi=120)\n",
    "plt.show()\n",
    "print('Scatter plot saved to hw3_output/actual_vs_predicted.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12717c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Section 6: Partial Dependence (Term Effect) Plots ─────────────────────\n",
    "# Plots how each predictor relates to fare while holding others at a reference\n",
    "# value.  95 % confidence intervals shown as shaded bands.\n",
    "\n",
    "TERM_LABELS = {\n",
    "    0: ('trip_distance',     'Trip Distance (miles)'),\n",
    "    1: ('trip_duration_min', 'Trip Duration (minutes)'),\n",
    "    2: ('hour_of_day',       'Hour of Day'),\n",
    "    3: ('day_of_week',       'Day of Week (0=Mon)'),\n",
    "    4: ('passenger_count',   'Passenger Count'),\n",
    "}\n",
    "if location_pca_joined:\n",
    "    TERM_LABELS[5] = ('pc1_score', 'Pickup Zone PC1 Score')\n",
    "    TERM_LABELS[6] = ('pc2_score', 'Pickup Zone PC2 Score')\n",
    "\n",
    "n_terms = len(TERM_LABELS)\n",
    "ncols = 3\n",
    "nrows = (n_terms + ncols - 1) // ncols\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4 * nrows))\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for idx, (term_i, (feat_name, xlabel)) in enumerate(TERM_LABELS.items()):\n",
    "    ax = axes[idx]\n",
    "    XX = gam.generate_X_grid(term=term_i)\n",
    "    pdep, confi = gam.partial_dependence(term=term_i, X=XX, width=0.95)\n",
    "    ax.plot(XX[:, term_i], pdep, color='steelblue', linewidth=2, label='Partial effect')\n",
    "    ax.fill_between(XX[:, term_i], confi[:, 0], confi[:, 1],\n",
    "                    alpha=0.3, color='steelblue', label='95% CI')\n",
    "    ax.axhline(0, color='grey', linewidth=0.8, linestyle='--')\n",
    "    ax.set_xlabel(xlabel, fontsize=11)\n",
    "    ax.set_ylabel('Partial Effect on Fare ($)', fontsize=10)\n",
    "    ax.set_title(f'Effect of {feat_name}', fontsize=12)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "# Hide any unused subplot panels\n",
    "for j in range(n_terms, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('GAM Partial Dependence Plots (95% CI)', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('partial_dependence.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Partial dependence plots saved to hw3_output/partial_dependence.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Extra Credit: Bootstrap CI Comparison ─────────────────────────────────\n",
    "# For 3 key terms (trip_distance, trip_duration_min, hour_of_day), we compare:\n",
    "#   (a) pygam's built-in 95 % CI (computed from the GAM covariance matrix)\n",
    "#   (b) Bootstrap 95 % CI (percentile method, N_BOOTSTRAP resamples)\n",
    "# This lets us assess whether pygam's analytical CIs are realistic.\n",
    "\n",
    "if not DO_BOOTSTRAP:\n",
    "    print('DO_BOOTSTRAP=False — skipping.')\n",
    "else:\n",
    "    BOOT_TERMS = [0, 1, 2]   # indices: trip_distance, trip_duration_min, hour_of_day\n",
    "    BOOT_LABELS = ['trip_distance', 'trip_duration_min', 'hour_of_day']\n",
    "\n",
    "    # Pre-compute evaluation grids from the full GAM\n",
    "    boot_grids   = [gam.generate_X_grid(term=t) for t in BOOT_TERMS]\n",
    "    # Storage: list of arrays shape (N_BOOTSTRAP, n_grid_points)\n",
    "    boot_curves  = [[] for _ in BOOT_TERMS]\n",
    "\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    n_train = len(X_train)\n",
    "\n",
    "    print(f'Running {N_BOOTSTRAP} bootstrap fits (this may take ~1-2 min)…')\n",
    "    for b in range(N_BOOTSTRAP):\n",
    "        idx = rng.integers(0, n_train, size=n_train)   # resample with replacement\n",
    "        Xb, yb = X_train[idx], y_train[idx]\n",
    "        try:\n",
    "            gam_b = LinearGAM(terms, fit_intercept=True)\n",
    "            # Fix lambdas to those found by gridsearch (faster, more stable)\n",
    "            gam_b.lam = gam.lam\n",
    "            gam_b.fit(Xb, yb)\n",
    "            for k, (term_i, grid) in enumerate(zip(BOOT_TERMS, boot_grids)):\n",
    "                pdep_b, _ = gam_b.partial_dependence(term=term_i, X=grid, width=0.95)\n",
    "                boot_curves[k].append(pdep_b)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if (b + 1) % 10 == 0:\n",
    "            print(f'  {b + 1}/{N_BOOTSTRAP} done')\n",
    "\n",
    "    print('Bootstrap complete.  Plotting CI comparison…')\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(BOOT_TERMS), figsize=(7 * len(BOOT_TERMS), 5))\n",
    "    if len(BOOT_TERMS) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for k, (term_i, feat_label) in enumerate(zip(BOOT_TERMS, BOOT_LABELS)):\n",
    "        ax = axes[k]\n",
    "        grid = boot_grids[k]\n",
    "        x_vals = grid[:, term_i]\n",
    "\n",
    "        # pygam analytic CI\n",
    "        pdep_main, confi_main = gam.partial_dependence(term=term_i, X=grid, width=0.95)\n",
    "\n",
    "        # Bootstrap percentile CI\n",
    "        curves_arr = np.array(boot_curves[k])  # shape (n_valid, n_grid)\n",
    "        boot_lo = np.percentile(curves_arr, 2.5,  axis=0)\n",
    "        boot_hi = np.percentile(curves_arr, 97.5, axis=0)\n",
    "\n",
    "        ax.plot(x_vals, pdep_main, color='steelblue', linewidth=2, label='GAM estimate')\n",
    "        ax.fill_between(x_vals, confi_main[:, 0], confi_main[:, 1],\n",
    "                        alpha=0.30, color='steelblue', label='pygam 95% CI')\n",
    "        ax.fill_between(x_vals, boot_lo, boot_hi,\n",
    "                        alpha=0.25, color='orange', label=f'Bootstrap 95% CI (B={len(curves_arr)})')\n",
    "        ax.set_xlabel(feat_label, fontsize=11)\n",
    "        ax.set_ylabel('Partial Effect ($)')\n",
    "        ax.set_title(f'CI Comparison: {feat_label}', fontsize=12)\n",
    "        ax.legend(fontsize=9)\n",
    "\n",
    "    plt.suptitle('Bootstrap vs. pygam Built-in 95% Confidence Intervals', fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bootstrap_ci_comparison.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Bootstrap CI comparison saved to hw3_output/bootstrap_ci_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb625a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Extra Credit: Per-Term Fare Component Breakdown ────────────────────────\n",
    "# Quantify how much each predictor contributes to the average predicted fare.\n",
    "# For each term i, we evaluate its partial_dependence at the training-set mean\n",
    "# of X and report the absolute contribution (additive effect).\n",
    "\n",
    "if not DO_FARE_BREAKDOWN:\n",
    "    print('DO_FARE_BREAKDOWN=False — skipping.')\n",
    "else:\n",
    "    # Use the validation set to measure average partial contributions\n",
    "    contributions = {}\n",
    "    for term_i, (feat_name, xlabel) in TERM_LABELS.items():\n",
    "        pdep, _ = gam.partial_dependence(term=term_i, X=X_val, width=0.95)\n",
    "        contributions[feat_name] = float(np.mean(np.abs(pdep)))\n",
    "\n",
    "    # Also include the fitted intercept as a separate component\n",
    "    intercept_val = float(gam.coef_[-1])   # last coefficient is intercept\n",
    "\n",
    "    labels = list(contributions.keys())\n",
    "    values = list(contributions.values())\n",
    "\n",
    "    # Sort descending\n",
    "    sorted_pairs = sorted(zip(values, labels), reverse=True)\n",
    "    values_sorted, labels_sorted = zip(*sorted_pairs)\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 0.8, len(labels_sorted)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    bars = ax.barh(labels_sorted, values_sorted, color=colors, edgecolor='white')\n",
    "    ax.set_xlabel('Mean |Partial Effect| on Fare ($)', fontsize=11)\n",
    "    ax.set_title('Per-Term Fare Component Breakdown\\n(mean absolute partial effect on validation set)', fontsize=12)\n",
    "    for bar, v in zip(bars, values_sorted):\n",
    "        ax.text(v + 0.02, bar.get_y() + bar.get_height() / 2,\n",
    "                f'${v:.2f}', va='center', fontsize=10)\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fare_component_breakdown.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print('\\nFare Component Breakdown (mean |partial effect| on validation set):')\n",
    "    for feat, val in sorted(contributions.items(), key=lambda x: -x[1]):\n",
    "        print(f'  {feat:25s}: ${val:.3f}')\n",
    "    print(f'  {\"intercept\":25s}: ${intercept_val:.3f}')\n",
    "    print('Component chart saved to hw3_output/fare_component_breakdown.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f25e0",
   "metadata": {},
   "source": [
    "## Discussion: Limitations\n",
    "\n",
    "The GAM provides a transparent and interpretable model for taxi fare prediction, but several\n",
    "limitations constrain its real-world accuracy:\n",
    "\n",
    "1. **Missing fare components**: The model intentionally excludes tolls, MTA taxes, congestion\n",
    "   surcharges, and airport fees — all of which add fixed or route-dependent charges to the\n",
    "   actual meter reading.  A trip through a toll zone will therefore be systematically\n",
    "   under-predicted.\n",
    "\n",
    "2. **No fine-grained GPS location**: Pickup/drop-off coordinates or street segments are not\n",
    "   used directly; instead, location is represented only through aggregate PCA scores derived\n",
    "   from the ride-count wide table.  Route geometry (e.g. highway vs. local streets) and\n",
    "   traffic conditions are therefore not captured.\n",
    "\n",
    "3. **Single-month snapshot**: The model is trained on January 2021 data only.  Seasonal demand\n",
    "   patterns, holiday effects, and post-pandemic ridership recovery trends visible in later\n",
    "   months are absent.\n",
    "\n",
    "4. **Passenger-count noise**: Many NYC TLC records have `passenger_count = 0` or suspiciously\n",
    "   high values; even after cleaning, this feature has low predictive power for fare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e1c39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EXTRA: Extended Analyses\n",
    "\n",
    "The three sections below go beyond the core assignment requirements.\n",
    "All figures are labeled **[EXTRA]** and all code blocks begin with the comment `# EXTRA`.\n",
    "\n",
    "| Section | What it adds |\n",
    "|---------|-------------|\n",
    "| **EXTRA 1 — Seaborn EDA** | Richer exploratory visuals using seaborn: fare distribution, fare vs hour violin, feature correlation heatmap |\n",
    "| **EXTRA 2 — Region Analysis** | Divide pickup/drop-off zones into NYC boroughs using `taxi_zones.csv` lat/lon bounding boxes; compare fare distributions and GAM fits per region |\n",
    "| **EXTRA 3 — Multi-year & Multi-service Comparison** | Load samples from 2015, 2021, 2022 yellow taxi + 2021 green taxi; compare fare distributions and partial-dependence curves across years and services |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610ebf3",
   "metadata": {},
   "source": [
    "## EXTRA 1 — Seaborn EDA\n",
    "\n",
    "Exploratory visualizations of the cleaned trip data using seaborn — fare distribution,\n",
    "fare vs hour-of-day, and a feature correlation heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f81826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA 1 — Seaborn EDA\n",
    "# Uses the cleaned `df` produced by Sections 3–4 above.\n",
    "# Requires seaborn (imported in cell 2).\n",
    "\n",
    "extra_df = df.copy()\n",
    "\n",
    "# ── 1a. Fare amount distribution ─────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "sns.histplot(extra_df['fare_amount'], bins=60, kde=True,\n",
    "             color='steelblue', ax=axes[0])\n",
    "axes[0].set_xlabel('Fare Amount ($)')\n",
    "axes[0].set_title('[EXTRA] Fare Amount Distribution')\n",
    "\n",
    "sns.histplot(np.log1p(extra_df['fare_amount']), bins=60, kde=True,\n",
    "             color='coral', ax=axes[1])\n",
    "axes[1].set_xlabel('log(1 + Fare Amount)')\n",
    "axes[1].set_title('[EXTRA] Log-Fare Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('extra_fare_distribution.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ── 1b. Fare vs hour of day (violin) ──────────────────────────────────────\n",
    "# Sample to keep rendering fast\n",
    "violin_sample = extra_df.sample(min(30_000, len(extra_df)), random_state=RANDOM_SEED)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.violinplot(data=violin_sample, x='hour_of_day', y='fare_amount',\n",
    "               palette='coolwarm', cut=0, linewidth=0.8, ax=ax)\n",
    "ax.set_xlabel('Hour of Day')\n",
    "ax.set_ylabel('Fare Amount ($)')\n",
    "ax.set_title('[EXTRA] Fare Distribution by Hour of Day')\n",
    "plt.tight_layout()\n",
    "plt.savefig('extra_fare_by_hour_violin.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ── 1c. Fare vs day of week (box) ─────────────────────────────────────────\n",
    "day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "violin_sample['day_label'] = violin_sample['day_of_week'].map(dict(enumerate(day_labels)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "sns.boxplot(data=violin_sample, x='day_label', y='fare_amount',\n",
    "            order=day_labels, palette='Set2', ax=ax)\n",
    "ax.set_xlabel('Day of Week')\n",
    "ax.set_ylabel('Fare Amount ($)')\n",
    "ax.set_title('[EXTRA] Fare Distribution by Day of Week')\n",
    "plt.tight_layout()\n",
    "plt.savefig('extra_fare_by_dow.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ── 1d. Feature correlation heatmap ──────────────────────────────────────\n",
    "corr_cols = ['fare_amount', 'trip_distance', 'trip_duration_min',\n",
    "             'hour_of_day', 'day_of_week', 'passenger_count']\n",
    "if location_pca_joined:\n",
    "    corr_cols += ['pc1_score', 'pc2_score']\n",
    "\n",
    "corr = extra_df[corr_cols].corr()\n",
    "fig, ax = plt.subplots(figsize=(len(corr_cols) + 1, len(corr_cols)))\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            square=True, linewidths=0.5, ax=ax)\n",
    "ax.set_title('[EXTRA] Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('extra_correlation_heatmap.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('EXTRA 1 figures saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9004a5",
   "metadata": {},
   "source": [
    "## EXTRA 2 — Region Analysis (Pickup/Drop-off by NYC Borough)\n",
    "\n",
    "NYC has 263 taxi zones spread across 5 boroughs. Analysing zone IDs alone makes interpretation\n",
    "difficult; grouping them into boroughs reveals spatial fare patterns that individual zone IDs obscure.\n",
    "\n",
    "**Method:** `taxi_zones.csv` provides a lat/lon centroid for each zone ID. We assign each zone to\n",
    "a borough using geographic bounding boxes, then join the borough label onto the trip data on\n",
    "`pickup_location`. We then compare fare distributions across boroughs and fit a separate GAM\n",
    "per borough to see how distance/duration effects differ spatially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57b802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA 2 — Region / Borough Assignment and Analysis\n",
    "\n",
    "ZONES_CSV_PATH = '../data/taxi_zones.csv'   # pickup_place, latitude, longitude\n",
    "\n",
    "# ── 2a. Assign borough to each zone using lat/lon bounding boxes ──────────\n",
    "def assign_borough(lat, lon):\n",
    "    \"\"\"\n",
    "    Approximate NYC borough from WGS-84 centroid.\n",
    "    Bounding boxes are intentionally conservative; ambiguous zones fall to\n",
    "    the 'Queens' default (largest borough by area).\n",
    "    \"\"\"\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return 'Unknown'\n",
    "    # Staten Island (far southwest)\n",
    "    if lon < -74.04:\n",
    "        return 'Staten Island'\n",
    "    # Bronx (north of Manhattan, slightly east)\n",
    "    if lat >= 40.795 and lon >= -73.935:\n",
    "        return 'Bronx'\n",
    "    # Manhattan (island: lon roughly -74.02 to -73.907, lat 40.70+)\n",
    "    if lat >= 40.695 and -74.02 <= lon <= -73.91:\n",
    "        return 'Manhattan'\n",
    "    # Brooklyn (south, west of Queens)\n",
    "    if lat < 40.715 and lon <= -73.84:\n",
    "        return 'Brooklyn'\n",
    "    # Everything else → Queens\n",
    "    return 'Queens'\n",
    "\n",
    "\n",
    "region_df = None\n",
    "try:\n",
    "    zones = pd.read_csv(ZONES_CSV_PATH)\n",
    "    zones.columns = [c.strip().lower() for c in zones.columns]\n",
    "    # columns expected: pickup_place / zone, latitude, longitude\n",
    "    if 'pickup_place' not in zones.columns:\n",
    "        zones = zones.rename(columns={zones.columns[0]: 'pickup_place'})\n",
    "\n",
    "    zones['pickup_place'] = pd.to_numeric(zones['pickup_place'], errors='coerce')\n",
    "    zones['borough'] = zones.apply(lambda r: assign_borough(r['latitude'], r['longitude']), axis=1)\n",
    "\n",
    "    print('Borough distribution across zones:')\n",
    "    print(zones['borough'].value_counts().to_string())\n",
    "\n",
    "    # Join onto trip data\n",
    "    if 'pickup_location' in df.columns:\n",
    "        region_df = df.copy()\n",
    "        region_df['pickup_location'] = pd.to_numeric(region_df['pickup_location'], errors='coerce')\n",
    "        region_df = region_df.merge(\n",
    "            zones[['pickup_place', 'borough']].rename(columns={'pickup_place': 'pickup_location'}),\n",
    "            on='pickup_location', how='left'\n",
    "        )\n",
    "        region_df['borough'] = region_df['borough'].fillna('Unknown')\n",
    "        match_pct = (region_df['borough'] != 'Unknown').mean() * 100\n",
    "        print(f'\\nTrips matched to a borough: {match_pct:.1f}%')\n",
    "        print(region_df['borough'].value_counts().to_string())\n",
    "    else:\n",
    "        print('pickup_location column not available — skipping join.')\n",
    "except Exception as e:\n",
    "    print(f'Could not load zones CSV ({e}) — EXTRA 2 region analysis skipped.')\n",
    "\n",
    "# ── 2b. Fare distribution by borough (violin + box) ───────────────────────\n",
    "if region_df is not None and 'borough' in region_df.columns:\n",
    "    BOROUGH_ORDER = ['Manhattan', 'Brooklyn', 'Queens', 'Bronx', 'Staten Island', 'Unknown']\n",
    "    present_boroughs = [b for b in BOROUGH_ORDER if b in region_df['borough'].values]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    sns.violinplot(data=region_df, x='borough', y='fare_amount',\n",
    "                   order=present_boroughs, palette='Set2', cut=0, linewidth=0.8,\n",
    "                   ax=axes[0])\n",
    "    axes[0].set_xlabel('NYC Borough (Pickup)')\n",
    "    axes[0].set_ylabel('Fare Amount ($)')\n",
    "    axes[0].set_title('[EXTRA] Fare Distribution by Pickup Borough')\n",
    "    axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "    # avg fare per borough × hour heatmap\n",
    "    pivot = (region_df[region_df['borough'].isin(present_boroughs) & (region_df['borough'] != 'Unknown')]\n",
    "             .groupby(['borough', 'hour_of_day'])['fare_amount']\n",
    "             .mean()\n",
    "             .unstack(fill_value=np.nan))\n",
    "    sns.heatmap(pivot.reindex([b for b in present_boroughs if b != 'Unknown']),\n",
    "                cmap='YlOrRd', linewidths=0.3, annot=False, ax=axes[1],\n",
    "                cbar_kws={'label': 'Mean Fare ($)'})\n",
    "    axes[1].set_xlabel('Hour of Day')\n",
    "    axes[1].set_ylabel('Borough')\n",
    "    axes[1].set_title('[EXTRA] Mean Fare by Borough × Hour')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('extra_fare_by_region.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ── 2c. GAM partial dependence for trip_distance, stratified by borough ──\n",
    "    from pygam import LinearGAM, s, l\n",
    "\n",
    "    borough_gams = {}\n",
    "    BOROUGH_COLORS = {\n",
    "        'Manhattan': '#e41a1c', 'Brooklyn': '#377eb8', 'Queens': '#4daf4a',\n",
    "        'Bronx': '#984ea3', 'Staten Island': '#ff7f00',\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    for bor in [b for b in present_boroughs if b != 'Unknown']:\n",
    "        sub = region_df[region_df['borough'] == bor].dropna(\n",
    "            subset=['trip_distance', 'trip_duration_min', 'fare_amount'])\n",
    "        if len(sub) < 200:\n",
    "            print(f'  Skipping {bor} — only {len(sub)} rows.')\n",
    "            continue\n",
    "\n",
    "        Xb = sub[['trip_distance', 'trip_duration_min', 'hour_of_day',\n",
    "                   'day_of_week', 'passenger_count']].values\n",
    "        yb = sub['fare_amount'].values\n",
    "        gam_b = LinearGAM(s(0) + s(1) + s(2, n_splines=24) + s(3, n_splines=7) + l(4),\n",
    "                          fit_intercept=True)\n",
    "        try:\n",
    "            gam_b.fit(Xb, yb)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        XX_b = gam_b.generate_X_grid(term=0)\n",
    "        pdep_b, _ = gam_b.partial_dependence(term=0, X=XX_b, width=0.95)\n",
    "        ax.plot(XX_b[:, 0], pdep_b, linewidth=2, label=bor,\n",
    "                color=BOROUGH_COLORS.get(bor, None))\n",
    "        borough_gams[bor] = gam_b\n",
    "\n",
    "    ax.set_xlabel('Trip Distance (miles)')\n",
    "    ax.set_ylabel('Partial Effect on Fare ($)')\n",
    "    ax.set_title('[EXTRA] Distance Effect on Fare — by NYC Borough\\n(separate GAM per borough)')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('extra_gam_by_region.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('EXTRA 2 figures saved.')\n",
    "else:\n",
    "    print('EXTRA 2 skipped (no region data).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e2254",
   "metadata": {},
   "source": [
    "## EXTRA 3 — Multi-year & Multi-service Fare Comparison (2015 · 2021 · 2022 · Green)\n",
    "\n",
    "NYC taxi fares changed markedly between 2015, 2021, and 2022:\n",
    "- **2015** — pre-app era; fares set by TLC meter.\n",
    "- **2021** — COVID recovery; lower ridership, same meter rates.\n",
    "- **2022** — post-COVID rebound; MTA congestion surcharge introduced in late 2021.\n",
    "\n",
    "We also include **green taxi** (2021) to compare fare structures between medallion yellow cabs\n",
    "(Manhattan-focused) and outer-borough green cabs.\n",
    "\n",
    "**Method:** Load a small sample (≤ 30k rows) from each source, normalize schemas, pool with a\n",
    "`service` label, and compare:\n",
    "1. Fare distributions (seaborn violin)\n",
    "2. Distance-vs-fare scatter (seaborn `lmplot`)\n",
    "3. GAM partial-dependence curves for `trip_distance` overlaid for all services/years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0386dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA 3 — Multi-year & Multi-service Fare Comparison\n",
    "\n",
    "# ── Dataset catalog ────────────────────────────────────────────────────────\n",
    "# Each entry: (label, S3 path, month to load)\n",
    "# All paths use the same anonymous S3 bucket pattern as the main data.\n",
    "# Paths that don't exist will be skipped gracefully.\n",
    "S3_BASE = 's3://dsc291-ucsd/taxi/Dataset'\n",
    "MULTI_SOURCES = [\n",
    "    ('Yellow 2015', f'{S3_BASE}/2015/yellow_taxi/yellow_tripdata_2015-01.parquet'),\n",
    "    ('Yellow 2021', f'{S3_BASE}/2021/yellow_taxi/yellow_tripdata_2021-01.parquet'),  # same as main\n",
    "    ('Yellow 2022', f'{S3_BASE}/2022/yellow_taxi/yellow_tripdata_2022-01.parquet'),\n",
    "    ('Green 2021',  f'{S3_BASE}/2021/green_taxi/green_tripdata_2021-01.parquet'),\n",
    "]\n",
    "MULTI_MAX_ROWS = 30_000   # per source — keep it fast\n",
    "\n",
    "# ── Helper functions (reuse normalize_columns from Section 3) ──────────────\n",
    "def load_and_normalize(label, path, max_rows=MULTI_MAX_ROWS, seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Load a parquet from local or S3, normalize columns, derive features.\n",
    "    Returns a small cleaned DataFrame with a 'service' column, or None on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        storage_options = {'anon': True} if path.startswith('s3://') else None\n",
    "        tmp = pd.read_parquet(path, storage_options=storage_options)\n",
    "        if len(tmp) > max_rows:\n",
    "            tmp = tmp.sample(n=max_rows, random_state=seed)\n",
    "        tmp = normalize_columns(tmp)\n",
    "        tmp['pickup_datetime']  = pd.to_datetime(tmp['pickup_datetime'],  errors='coerce')\n",
    "        tmp['dropoff_datetime'] = pd.to_datetime(tmp['dropoff_datetime'], errors='coerce')\n",
    "        tmp['trip_duration_min'] = (\n",
    "            (tmp['dropoff_datetime'] - tmp['pickup_datetime']).dt.total_seconds() / 60.0)\n",
    "        tmp['hour_of_day'] = tmp['pickup_datetime'].dt.hour\n",
    "        tmp['day_of_week'] = tmp['pickup_datetime'].dt.dayofweek\n",
    "        if 'passenger_count' not in tmp.columns:\n",
    "            tmp['passenger_count'] = 1\n",
    "        tmp['passenger_count'] = pd.to_numeric(tmp['passenger_count'], errors='coerce').fillna(1)\n",
    "        # Clean\n",
    "        tmp = tmp.dropna(subset=['fare_amount', 'trip_distance', 'trip_duration_min'])\n",
    "        tmp = tmp[(tmp['fare_amount'] > 0) & (tmp['trip_distance'] > 0) &\n",
    "                  (tmp['trip_duration_min'] > 0)]\n",
    "        for col in ['fare_amount', 'trip_distance', 'trip_duration_min']:\n",
    "            p99 = tmp[col].quantile(0.99)\n",
    "            tmp = tmp[tmp[col] <= p99]\n",
    "        tmp['service'] = label\n",
    "        print(f'  {label:15s}: {len(tmp):,} rows loaded from {path}')\n",
    "        return tmp\n",
    "    except Exception as exc:\n",
    "        print(f'  {label:15s}: SKIPPED — {exc}')\n",
    "        return None\n",
    "\n",
    "\n",
    "print('Loading multi-year / multi-service samples...')\n",
    "frames = [load_and_normalize(lbl, pth) for lbl, pth in MULTI_SOURCES]\n",
    "frames = [f for f in frames if f is not None]\n",
    "\n",
    "if len(frames) < 2:\n",
    "    print('\\nFewer than 2 sources loaded — skipping EXTRA 3 comparisons.')\n",
    "else:\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    print(f'\\nCombined: {len(combined):,} rows across {combined[\"service\"].nunique()} services.')\n",
    "\n",
    "    SERVICE_ORDER = [lbl for lbl, _ in MULTI_SOURCES\n",
    "                     if any(f['service'].iloc[0] == lbl for f in frames)]\n",
    "\n",
    "    # ── 3a. Fare distribution violin by service ────────────────────────────\n",
    "    fig, ax = plt.subplots(figsize=(11, 5))\n",
    "    sns.violinplot(data=combined, x='service', y='fare_amount',\n",
    "                   order=SERVICE_ORDER, palette='tab10', cut=0, linewidth=0.8, ax=ax)\n",
    "    ax.set_xlabel('Service / Year')\n",
    "    ax.set_ylabel('Fare Amount ($)')\n",
    "    ax.set_title('[EXTRA] Fare Distribution by Year & Service\\n(January samples, n≤30k each)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('extra_fare_by_service.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ── 3b. Median fare statistics table ──────────────────────────────────\n",
    "    stats = (combined.groupby('service')['fare_amount']\n",
    "             .agg(n='count', mean='mean', median='median', std='std', p25=lambda x: x.quantile(0.25),\n",
    "                  p75=lambda x: x.quantile(0.75))\n",
    "             .round(2))\n",
    "    print('\\nFare statistics by service:')\n",
    "    display(stats)\n",
    "\n",
    "    # ── 3c. Distance vs fare scatter (seaborn lmplot, log-log) ────────────\n",
    "    sample_comb = combined.sample(min(8_000, len(combined)), random_state=RANDOM_SEED)\n",
    "    sample_comb['log_dist'] = np.log1p(sample_comb['trip_distance'])\n",
    "    sample_comb['log_fare'] = np.log1p(sample_comb['fare_amount'])\n",
    "\n",
    "    g = sns.FacetGrid(sample_comb, col='service', col_order=SERVICE_ORDER,\n",
    "                      col_wrap=2, height=4, sharey=True, sharex=True)\n",
    "    g.map_dataframe(sns.scatterplot, x='log_dist', y='log_fare',\n",
    "                    alpha=0.15, s=8, color='steelblue')\n",
    "    g.map_dataframe(sns.regplot, x='log_dist', y='log_fare',\n",
    "                    scatter=False, color='crimson', line_kws={'linewidth': 1.5})\n",
    "    g.set_axis_labels('log(1 + Distance)', 'log(1 + Fare)')\n",
    "    g.set_titles(col_template='[EXTRA] {col_name}')\n",
    "    g.figure.suptitle('log(Distance) vs log(Fare) by Service', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('extra_distance_vs_fare_by_service.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ── 3d. GAM distance partial dependence overlaid by service ───────────\n",
    "    print('\\nFitting one GAM per service for partial-dependence comparison...')\n",
    "    BASE_TERMS = s(0) + s(1) + s(2, n_splines=24) + s(3, n_splines=7) + l(4)\n",
    "    PALETTE = sns.color_palette('tab10', n_colors=len(SERVICE_ORDER))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "    for color, svc in zip(PALETTE, SERVICE_ORDER):\n",
    "        sub = combined[combined['service'] == svc].copy()\n",
    "        feat_cols = ['trip_distance', 'trip_duration_min', 'hour_of_day',\n",
    "                     'day_of_week', 'passenger_count']\n",
    "        sub = sub.dropna(subset=feat_cols + ['fare_amount'])\n",
    "        if len(sub) < 200:\n",
    "            continue\n",
    "        Xs = sub[feat_cols].values\n",
    "        ys = sub['fare_amount'].values\n",
    "        gam_s = LinearGAM(BASE_TERMS, fit_intercept=True)\n",
    "        try:\n",
    "            gam_s.fit(Xs, ys)\n",
    "        except Exception as e:\n",
    "            print(f'  GAM failed for {svc}: {e}')\n",
    "            continue\n",
    "\n",
    "        XX_s = gam_s.generate_X_grid(term=0)\n",
    "        pdep_s, confi_s = gam_s.partial_dependence(term=0, X=XX_s, width=0.95)\n",
    "        ax.plot(XX_s[:, 0], pdep_s, linewidth=2, label=svc, color=color)\n",
    "        ax.fill_between(XX_s[:, 0], confi_s[:, 0], confi_s[:, 1],\n",
    "                        alpha=0.15, color=color)\n",
    "\n",
    "    ax.set_xlabel('Trip Distance (miles)')\n",
    "    ax.set_ylabel('Partial Effect on Fare ($)')\n",
    "    ax.set_title('[EXTRA] Distance Effect on Fare — GAM Comparison\\nYellow 2015 vs 2021 vs 2022 vs Green 2021')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('extra_gam_distance_by_service.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print('EXTRA 3 figures saved.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
